{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recycling Robot Example\n",
    "\n",
    "This notebook is inspired by the \"*Recycling Robot*\" example from [Sutton and Barto, Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).\n",
    "\n",
    "Suppose we have a robot going anywhere in an office searching for empty cans to trash them into a bin. The robot has three states $\\mathcal{S} = \\{\\texttt{high},\\texttt{low},\\texttt{broken}\\}$ indicating its rechargerable battery status. In each state, the robot can decide among three actions $\\mathcal{S} = \\{\\texttt{search},\\texttt{wait},\\texttt{recharge}\\}$, indicating respectively searching for empty cans in the office, remain stationary and wait, or head back to its charging base and recharge. Depending on the current state $s \\in \\mathcal{S}$, the robot can choose among varying sets of actions. In our case,\n",
    "$$\\mathcal{A}(\\texttt{high}) = \\{\\texttt{search},\\texttt{wait}\\}$$\n",
    "$$\\mathcal{A}(\\texttt{low}) = \\{\\texttt{search},\\texttt{wait},\\texttt{recharge}\\}$$\n",
    "$$\\mathcal{A}(\\texttt{broken}) = \\emptyset$$\n",
    "\n",
    "The rewards are zero most of the time, but become positive when the robot secures an empty can, or large and negative if the battery runs all the way down. Here is a diagram of the dynamics of this environment:\n",
    "\n",
    "<img src=\"img/recycling-robot.svg\" />\n",
    "\n",
    "Below is how this can be implemented with our package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_search = 3\n",
    "r_wait = 1\n",
    "r_depleted = -3\n",
    "r_broken = 0\n",
    "alpha = 0.4\n",
    "beta = 0.6\n",
    "gamma = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `relearn` Implementation\n",
    "\n",
    "In order to have a complete Markov Decision Process (MDP) to test we first need to create its building blocks, namely the Environment and the Agent. In turn, the Environment needs States, Actions, Rewards and Transitions to run (see the picture above to get an idea of the Environment's elements), while the Agent needs a Policy. Let's define these objects, and then we will put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relearn.agent import *\n",
    "from relearn.environment import *\n",
    "from relearn.mdp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Environment: Actions, States, Rewards, and Transitions\n",
    "\n",
    "As just said, we need to declare a few objects for instantiating an Environment. The `Action`, `State` and `Rewards` classes are quite simple, and can be inherited by more complex structures if needed. To instantiate them, we need unique names to assign to each action and state and unique values to assign to rewards. Additionally, the end and start states can be set. If either none or a combination is set, the default behaviour is to assign the first inserted stated the 'start state' status, while the 'end state' status remains unassigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [Action(name) for name in [\"search\", \"wait\", \"recharge\"]]\n",
    "states = [State(name) for name in [\"low\", \"high\", \"broken\"]]\n",
    "states[-1].end_state = True  # sets the state 'broken' as end state\n",
    "states[1].start_state = True  # sets the state 'high' as start state\n",
    "rewards = [Reward(value) for value in [r_search, r_wait, r_broken, r_depleted]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have actions, states and rewards, we can also instantiate transitions using the `Transition` class. Each transition is made of 5 objects: \n",
    "\n",
    "- the starting state, \n",
    "- the action taken in the starting state, \n",
    "- the ending state, \n",
    "- the reward given by performing the selected action from the starting state and landing in the ending state, \n",
    "- and the probability of this transition to occur given the action taken in the starting state.\n",
    "\n",
    "We can specify each of these objects by referring to names and values given to actions, states, and rewards, respectively. We have to pay attention here because if the names or the reward values do not coincide with the intended object, the Environment class won't accept them as valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = [\n",
    "    Transition(\n",
    "        start_state_name=\"low\",\n",
    "        action_name=\"search\",\n",
    "        landing_state_name=\"low\",\n",
    "        reward_value=r_search,\n",
    "        probability=beta,\n",
    "    ),\n",
    "    Transition(\"low\", \"search\", \"high\", r_depleted, 1 - beta - gamma),\n",
    "    Transition(\"low\", \"search\", \"broken\", r_broken, gamma),\n",
    "    Transition(\"low\", \"wait\", \"low\", r_wait, 1 - gamma),\n",
    "    Transition(\"low\", \"wait\", \"broken\", r_broken, gamma),\n",
    "    Transition(\"low\", \"recharge\", \"high\", 0, 1 - gamma),\n",
    "    Transition(\"low\", \"recharge\", \"broken\", r_broken, gamma),\n",
    "    Transition(\"high\", \"search\", \"low\", r_search, 1 - alpha - gamma),\n",
    "    Transition(\"high\", \"search\", \"high\", r_search, alpha),\n",
    "    Transition(\"high\", \"search\", \"broken\", r_broken, gamma),\n",
    "    Transition(\"high\", \"wait\", \"high\", r_wait, 1 - gamma),\n",
    "    Transition(\"high\", \"wait\", \"broken\", r_broken, gamma),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Environment object can be finally instantiated using actions, states, rewards and transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment(\n",
    "    states=states, actions=actions, rewards=rewards, transitions=transitions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Agent and the Policy\n",
    "\n",
    "The `Agent` class is quite trivial in the sense that its only scope is to make the policy run. Thus, its implementation simply consists in calling the `Agent` constructor and giving it a `Policy`. There is a convenient static method in the `Policy` class to set a random policy given the shape of its probability distribution, which should be `n_states x n_actions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = Policy.random_policy(\n",
    "    n_states=len(environment.states), n_actions=len(environment.actions)\n",
    ")\n",
    "agent = Agent(policy=random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together in a MDP\n",
    "\n",
    "In order to make everything work, the `MDP` class is designed to orchestrate and manage all the previous objects seemlessly, allowing for MDP iterations and trajectory retrieval. In order to go reverse an iteration, the trajectory object must be edited so that it does not contain traces of the n-last iteration(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(agent=agent, environment=environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Now we have everything to run iterations and see interactions between the agent and the environment that we have just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0\tState: high\tAction: search\n",
      "Reward: 3\tState: low\n"
     ]
    }
   ],
   "source": [
    "mdp.iterate()\n",
    "mdp.print_trajectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are in state low, and we have just received the reward 3\n"
     ]
    }
   ],
   "source": [
    "print(f'We are in state {mdp.current_state.name}, and we have just received the reward {mdp.trajectory[\"rewards\"][-1].value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `trajectory` object is a dictionary containing three keys (states, actions and rewards), each of which has its own list containing the states, actions and rewards experienced at time $i$, where $i$ is the index in those lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards': [<relearn.environment.Reward object at 0x1081c9090>, <relearn.environment.Reward object at 0x1081cbac0>], 'states': [<relearn.environment.State object at 0x1081cbc10>, <relearn.environment.State object at 0x109033c10>], 'actions': [<relearn.environment.Action object at 0x1081c9900>]}\n"
     ]
    }
   ],
   "source": [
    "print(mdp.trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming: Iterative Policy Evaluation\n",
    "\n",
    "One of the most important computations in RL is the estimation of the value function $v_\\pi$, which gives the expected value of each state (refer to [Sutton and Barto, Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for theory and pseudocode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between old value function and new value function: 0.6982845496808725\n",
      "Difference between old value function and new value function: 0.0035326165312502544\n",
      "Difference between old value function and new value function: 1.8970495003878263e-05\n"
     ]
    }
   ],
   "source": [
    "theta = 0.0001\n",
    "discount = 0.01\n",
    "\n",
    "# initialize V(s), for all s in S+, arbitrarily except that V(terminal)=0\n",
    "value_function = np.random.rand(len(mdp.environment.states))\n",
    "\n",
    "# set V(terminal)=0, if there is end state\n",
    "terminal = [s.idx for s in mdp.environment.states if s.end_state]\n",
    "if terminal:\n",
    "    value_function[terminal] = 0\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    for state in mdp.environment.states:\n",
    "        old_v = value_function[state.idx].copy()\n",
    "        value_function[state.idx] = sum(\n",
    "            mdp.agent.policy.pmf(action=action, state=state)\n",
    "            * sum(\n",
    "                mdp.environment.state_reward_proba(\n",
    "                    next_state=next_state,\n",
    "                    reward=reward,\n",
    "                    state=state,\n",
    "                    action=action,\n",
    "                )\n",
    "                * (reward.value + discount * value_function[next_state.idx])\n",
    "                for next_state in mdp.environment.states\n",
    "                for reward in mdp.environment.rewards\n",
    "            )\n",
    "            for action in state.actions\n",
    "        )\n",
    "        delta = max([delta, np.abs(old_v - value_function[state.idx])])\n",
    "    print(f\"Difference between old value function and new value function: {delta}\")\n",
    "    if delta < theta: # this process stops if the accuracy threshold theta is met\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Greedy Policy computation with Dynamic Programming\n",
    "\n",
    "Let's see how we can implement a RL algorithm, namely the computation of a new greedy policy starting from a random one (refer to [Sutton and Barto, Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for theory and pseudocode).\n",
    "\n",
    "We need to specify the state from which we want our policy to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67773438 0.31860352 0.0033493 ]\n"
     ]
    }
   ],
   "source": [
    "state = mdp.current_state\n",
    "discount = 0.01\n",
    "\n",
    "# compute the expected rewards of selecting each action and then follow\n",
    "# the policy\n",
    "new_expected_rewards = np.array(\n",
    "    [\n",
    "        sum(\n",
    "            mdp.environment.state_reward_proba(\n",
    "                next_state=next_state,\n",
    "                reward=reward,\n",
    "                state=state,\n",
    "                action=action,\n",
    "            )\n",
    "            * (reward.value + discount * value_function[next_state.idx])\n",
    "            for next_state in mdp.environment.states for reward in mdp.environment.rewards\n",
    "        )\n",
    "        for action in state.actions\n",
    "    ]\n",
    ")\n",
    "\n",
    "# assign to each action new probabilities with respect to those actions\n",
    "# with highest expected sum of rewards. Making sure everything is in (0,1)\n",
    "# excluding extremes\n",
    "probas = new_expected_rewards + np.abs(np.min(new_expected_rewards)) + 0.000001\n",
    "probas = np.float16(probas) / np.sum(probas)\n",
    "new_probas = np.zeros(len(mdp.environment.actions))\n",
    "new_probas[[action.idx for action in state.actions]] = probas\n",
    "print(new_probas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-learning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
