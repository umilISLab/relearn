{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recycling Robot Example\n",
    "\n",
    "This notebook is inspired by the \"*Recycling Robot*\" example from [Sutton and Barto, Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).\n",
    "\n",
    "Suppose we have a robot going anywhere in an office searching for empty cans to trash them into a bin. The robot has three states $\\mathcal{S} = \\{\\texttt{high},\\texttt{low},\\texttt{broken}\\}$ indicating its rechargerable battery status. In each state, the robot can decide among three actions $\\mathcal{A} = \\{\\texttt{search},\\texttt{wait},\\texttt{recharge}\\}$, indicating respectively searching for empty cans in the office, remain stationary and wait, or head back to its charging base and recharge. Depending on the current state $s \\in \\mathcal{S}$, the robot can choose among varying sets of actions. In our case,\n",
    "$$\\mathcal{A}(\\texttt{high}) = \\{\\texttt{search},\\texttt{wait}\\}$$\n",
    "$$\\mathcal{A}(\\texttt{low}) = \\{\\texttt{search},\\texttt{wait},\\texttt{recharge}\\}$$\n",
    "$$\\mathcal{A}(\\texttt{broken}) = \\emptyset$$\n",
    "\n",
    "The rewards are zero most of the time, but become positive when the robot secures an empty can, or large and negative if the battery runs all the way down. Here is a diagram of the dynamics of this environment:\n",
    "\n",
    "<img src=\"img/recycling_robot.svg\" />\n",
    "\n",
    "Below is how this can be implemented with our package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_search = 3\n",
    "r_wait = 1\n",
    "r_depleted = -3 # when the robot depletes its battery before reaching the charging station\n",
    "r_broken = 0\n",
    "alpha = 0.4\n",
    "beta = 0.6\n",
    "gamma = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `relearn` Implementation\n",
    "\n",
    "In order to have a complete Markov Decision Process (MDP) to test we first need to create its building blocks, namely the Environment and the Agent. In turn, the Environment needs States, Actions, Rewards and Transitions to run (see the picture above to get an idea of the Environment's elements), while the Agent needs a Policy. Let's define these objects, and then we will put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/Users/lizzy/research/relearn/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relearn.policy import *\n",
    "from relearn.environment import *\n",
    "from relearn.mdp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Environment: Actions, States, Rewards, and Transitions\n",
    "\n",
    "As just said, we need to declare a few objects for instantiating an Environment. The `Action`, `State` and `Rewards` classes are quite simple, and can be inherited by more complex structures if needed. To instantiate them, we need unique names to assign to each action and state and unique values to assign to rewards. Additionally, the end and start states can be set. If either none or a combination is set, the default behaviour is to assign the first inserted stated the 'start state' status, while the 'end state' status remains unassigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [Action(name) for name in [\"search\", \"wait\", \"recharge\"]]\n",
    "states = [State(name) for name in [\"low\", \"high\", \"broken\"]]\n",
    "states[-1].end_state = True  # sets the state 'broken' as end state\n",
    "states[1].start_state = True  # sets the state 'high' as start state\n",
    "rewards = [Reward(value) for value in [r_search, r_wait, r_broken, r_depleted]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have actions, states and rewards, we can also instantiate transitions using the `Transition` class. Each transition is made of 5 objects: \n",
    "\n",
    "- the starting state, \n",
    "- the action taken in the starting state, \n",
    "- the ending state, \n",
    "- the reward given by performing the selected action from the starting state and landing in the ending state, \n",
    "- and the probability of this transition to occur given the action taken in the starting state.\n",
    "\n",
    "We can specify each of these objects by referring to names and values given to actions, states, and rewards, respectively. We have to pay attention here because if the names or the reward values do not coincide with the intended object, the Environment class won't accept them as valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = [\n",
    "    Transition(\n",
    "        start_state_name=\"low\",\n",
    "        action_name=\"search\",\n",
    "        landing_state_name=\"low\",\n",
    "        reward_value=r_search,\n",
    "        probability=beta,\n",
    "    ),\n",
    "    Transition(\"low\", \"search\", \"broken\", r_depleted, 1-beta),\n",
    "    Transition(\"low\", \"wait\", \"low\", r_wait, 1 - gamma),\n",
    "    Transition(\"low\", \"wait\", \"broken\", r_broken, gamma),\n",
    "    Transition(\"low\", \"recharge\", \"high\", 0, 1 - gamma),\n",
    "    Transition(\"low\", \"recharge\", \"broken\", r_broken, gamma),\n",
    "    Transition(\"high\", \"search\", \"low\", r_search, 1 - alpha - gamma),\n",
    "    Transition(\"high\", \"search\", \"high\", r_search, alpha),\n",
    "    Transition(\"high\", \"search\", \"broken\", r_broken, gamma),\n",
    "    Transition(\"high\", \"wait\", \"high\", r_wait, 1 - gamma),\n",
    "    Transition(\"high\", \"wait\", \"broken\", r_broken, gamma),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Environment object can be finally instantiated using actions, states, rewards and transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m environment \u001b[38;5;241m=\u001b[39m \u001b[43mEnvironment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransitions\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/relearn/relearn/environment.py:217\u001b[0m, in \u001b[0;36mEnvironment.__init__\u001b[0;34m(self, states, actions, rewards, transitions)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_state:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamics \u001b[38;5;241m=\u001b[39m \u001b[43minitialise_dynamics_from_quadruples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrewards\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates:\n\u001b[1;32m    222\u001b[0m     state\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[a]\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39margwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamics[state\u001b[38;5;241m.\u001b[39midx])[:, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    225\u001b[0m     ]\n",
      "File \u001b[0;32m~/research/relearn/relearn/environment.py:146\u001b[0m, in \u001b[0;36minitialise_dynamics_from_quadruples\u001b[0;34m(transitions_list, states, actions, rewards)\u001b[0m\n\u001b[1;32m    139\u001b[0m     reward_idx \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    140\u001b[0m         reward\u001b[38;5;241m.\u001b[39midx \u001b[38;5;28;01mfor\u001b[39;00m reward \u001b[38;5;129;01min\u001b[39;00m rewards \u001b[38;5;28;01mif\u001b[39;00m reward\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m transition\u001b[38;5;241m.\u001b[39mreward\n\u001b[1;32m    141\u001b[0m     ]\n\u001b[1;32m    142\u001b[0m     dynamics[start_state_idx, action_idx, end_state_idx, reward_idx] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    143\u001b[0m         transition\u001b[38;5;241m.\u001b[39mprobability\n\u001b[1;32m    144\u001b[0m     )\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mProbabilityArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_zero_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/relearn/relearn/utils.py:62\u001b[0m, in \u001b[0;36mProbabilityArray.__new__\u001b[0;34m(cls, input_array, allow_zero_vectors)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, input_array, allow_zero_vectors: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new ProbabilityArray instance, ensuring it represents a valid\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    probability distribution.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m        ValueError: If the input_array contains negative values or does not sum to 1.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_array\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_is_probability_distribution():\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbabilities are either negative or their sum is not close to one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         )\n",
      "File \u001b[0;32m~/research/relearn/relearn/utils.py:76\u001b[0m, in \u001b[0;36mProbabilityArray.__array_finalize__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# pylint: disable=W0201\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_zero_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_zero_vectors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/research/relearn/relearn/utils.py:76\u001b[0m, in \u001b[0;36mProbabilityArray.__array_finalize__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# pylint: disable=W0201\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_zero_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_zero_vectors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-learning-venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-learning-venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-learning-venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-learning-venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "environment = Environment(\n",
    "    states=states, actions=actions, rewards=rewards, transitions=transitions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Agent and the Policy\n",
    "\n",
    "The `Agent` class is quite trivial in the sense that its only scope is to make the policy run. Thus, its implementation simply consists in calling the `Agent` constructor and giving it a `Policy`. There is a convenient static method in the `Policy` class to set a random policy given the shape of its probability distribution, which should be `n_states x n_actions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = Policy.random_policy(\n",
    "    n_states=len(environment.states), actions=environment.actions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together in a MDP\n",
    "\n",
    "In order to make everything work, the `MDP` class is designed to orchestrate and manage all the previous objects seemlessly, allowing for MDP iterations and trajectory retrieval. In order to go reverse an iteration, the trajectory object must be edited so that it does not contain traces of the n-last iteration(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(policy=random_policy, environment=environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Now we have everything to run iterations and see interactions between the agent and the environment that we have just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0\tState: high\tAction: search\n",
      "Reward: 3\tState: high\n"
     ]
    }
   ],
   "source": [
    "mdp.iterate()\n",
    "mdp.print_trajectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are in state high, and we have just received the reward 3\n"
     ]
    }
   ],
   "source": [
    "print(f'We are in state {mdp.current_state.name}, and we have just received the reward {mdp.trajectory[\"rewards\"][-1].value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `trajectory` object is a dictionary containing three keys (states, actions and rewards), each of which has its own list containing the states, actions and rewards experienced at time $i$, where $i$ is the index in those lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards': [<relearn.environment.Reward object at 0x10672e620>, <relearn.environment.Reward object at 0x10672cee0>], 'states': [<relearn.environment.State object at 0x10672f9a0>, <relearn.environment.State object at 0x10672f9a0>], 'actions': [<relearn.environment.Action object at 0x10672d450>]}\n"
     ]
    }
   ],
   "source": [
    "print(mdp.trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming: Iterative Policy Evaluation\n",
    "\n",
    "One of the most important computations in RL is the estimation of the value function $v_\\pi$, which gives the expected value of each state (refer to [Sutton and Barto, Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for theory and pseudocode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between old value function and new value function: 0.34983012257517304\n",
      "Difference between old value function and new value function: 0.0018919797737135258\n",
      "Difference between old value function and new value function: 8.709769357129193e-06\n"
     ]
    }
   ],
   "source": [
    "theta = 0.0001\n",
    "discount = 0.01\n",
    "\n",
    "# initialize V(s), for all s in S+, arbitrarily except that V(terminal)=0\n",
    "value_function = np.random.rand(len(mdp.environment.states))\n",
    "\n",
    "# set V(terminal)=0, if there is end state\n",
    "terminal = [s.idx for s in mdp.environment.states if s.end_state]\n",
    "if terminal:\n",
    "    value_function[terminal] = 0\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    for state in mdp.environment.states:\n",
    "        old_v = value_function[state.idx].copy()\n",
    "        value_function[state.idx] = sum(\n",
    "            mdp.policy.pmf(action=action, state=state)\n",
    "            * sum(\n",
    "                mdp.environment.state_reward_proba(\n",
    "                    next_state=next_state,\n",
    "                    reward=reward,\n",
    "                    state=state,\n",
    "                    action=action,\n",
    "                )\n",
    "                * (reward.value + discount * value_function[next_state.idx])\n",
    "                for next_state in mdp.environment.states\n",
    "                for reward in mdp.environment.rewards\n",
    "            )\n",
    "            for action in state.actions\n",
    "        )\n",
    "        delta = max([delta, np.abs(old_v - value_function[state.idx])])\n",
    "    print(f\"Difference between old value function and new value function: {delta}\")\n",
    "    if delta < theta: # this process stops if the accuracy threshold theta is met\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Greedy Policy computation with Dynamic Programming\n",
    "\n",
    "Let's see how we can implement a RL algorithm, namely the computation of a new greedy policy starting from a random one (refer to [Sutton and Barto, Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for theory and pseudocode).\n",
    "\n",
    "We need to specify the state from which we want our policy to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66552734 0.33398438 0.        ]\n"
     ]
    }
   ],
   "source": [
    "state = mdp.current_state\n",
    "discount = 0.01\n",
    "\n",
    "# compute the expected rewards of selecting each action and then follow\n",
    "# the policy\n",
    "new_expected_rewards = np.array(\n",
    "    [\n",
    "        sum(\n",
    "            mdp.environment.state_reward_proba(\n",
    "                next_state=next_state,\n",
    "                reward=reward,\n",
    "                state=state,\n",
    "                action=action,\n",
    "            )\n",
    "            * (reward.value + discount * value_function[next_state.idx])\n",
    "            for next_state in mdp.environment.states for reward in mdp.environment.rewards\n",
    "        )\n",
    "        for action in state.actions\n",
    "    ]\n",
    ")\n",
    "\n",
    "# assign to each action new probabilities with respect to those actions\n",
    "# with highest expected sum of rewards. Making sure everything is in (0,1)\n",
    "# excluding extremes\n",
    "probas = ProbabilityArray.from_any_real_array(new_expected_rewards)\n",
    "new_probas = np.zeros(len(mdp.environment.actions))\n",
    "new_probas[[action.idx for action in state.actions]] = probas\n",
    "print(new_probas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-learning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
